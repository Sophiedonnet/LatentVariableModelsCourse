---
title: "Chapter 1.  PalmerPinguins dataset"
author: "Sophie Donnet"
date: "2025-01-07"
bibliography: /home/donnet/WORK_ALL/ENSEIGNEMENT/2025-Saclay-MathsSV/IllustrationsRmd/biblioBook.bib
output: 
  html_document:
   self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, message=FALSE, warning=FALSE}
library(ggplot2)
library(parallel)
library(viridis)
library(dplyr)
library(knitr)
```

## 1.  The Palmer penguin  dataset


<img src="PenguinAdeliephoto.jpg" alt="Adelie Penguins" width="20%" />


 




As a first illustrative example, let us consider the Palmer penguin  dataset @palmerpenguins.   The dataset contains several physical measurements collected on  $342$ penguins   from three islands in the Palmer Archipelago, Antarctica.. 


```{r data, message=FALSE, warning=FALSE}
 
library(palmerpenguins)
head(penguins)
``` 


We focus on the measurement of bill length for $342$ penguins.   



```{r data plot, message=FALSE, warning=FALSE}

library(palmerpenguins)

gg <- ggplot(penguins,aes(x=bill_length_mm,y = after_stat(density))) 
gg <- gg + geom_histogram(color = "white",fill="cyan4")
gg
```          

## 2. EM algorithm for 2 univariate gaussian distributions 



### 2.a Functions  
For `params` a list of 
    
  - `params$p`: $p$,  the proportion of cluster 1
  
  - ` params$mu` $(\mu_1,\mu_2)$, the 2 means
    
  - `params$var` : $(\sigma^2_1, \sigma^2_2)$, the two variances  

we write the following functions 
 
  - `logLikelihood = function(params,y)`
  - `EM_2Mixture =   function(params0,y,tol=10^{-5},estim_only_mu=FALSE)`
  
  
  
```{r source EM }
source('Functions_EM.R')
y <- penguins$bill_length_mm
y <- y[!is.na(y)]

```




### 2.b Run the  EM  



We now run the EM from various starting points. 

```{r starting points}

KM <- kmeans(y, 2)

params0 = list(list(mu=c(40,50),var=c(5,5),p=0.5),
               list(mu=c(20,50),var=c(5,5),p=0.5),
               list(mu=c(35,70),var=c(5,5),p=0.6),
               list(mu=c(50,40),var=c(10,10),p=0.4))
params0[[5]] = list(mu=c(40,50),var=c(1,1),p=0.5)
params0[[6]] = list(mu=KM$centers,var=c(3,3),p=0.5)
res_all_run_all <-do.call(rbind,lapply(1:length(params0),function(run){
  res_EM_run  <- EM_2Mixture(params0[[run]],y,tol=10^{-7},estim_only_mu = FALSE)
  niter <- nrow(res_EM_run)
  res_EM_run <- cbind(res_EM_run,rep(run,niter))
  res_EM_run <- cbind(res_EM_run, c(1:nrow(res_EM_run)))
  names(res_EM_run)[7]='Init'
  names(res_EM_run)[8]='numIter'
  
  return(res_EM_run)}))
```


The following table provides 5 initial points $\theta^{(0)}$ and the likelihood reached by the EM starting from theses points. The initial points 1,2,4,5, and 6 lead to the same value of the likelihood ($p_{\widehat{\theta}}(y)=-1043.56$) and to the same value of parameter $\widehat{\theta}$ (not shown here).  However, the third initial point does not allow the EM algorithm to reach the global maximum. 




```{r echo  = FALSE}
#------------------------------------------------
tabl <- matrix(unlist(params0),ncol=5,byrow=TRUE)
tabl <- cbind(tabl,rep(0,length(params0)))
row.names(tabl) = 1:length(params0)
for (init in 1:length(params0)){
  w <- which(res_all_run_all$Init==init)
  tabl[init,6]<- max(res_all_run_all$loglik[w])
}
colnames(tabl) <- c('mu_1','mu_2','sigma_1_2','sigma_2_2','p','ll'  )
kable(tabl)
```

We plot the trajectories of the log-likelihood along the iterations of the algorithm starting from the  initial points $\theta^{(0)}$ reported in the table. 


```{r EM plot ll, eval=TRUE, echo = FALSE}

res_all_run_all$Init <- as.factor(res_all_run_all$Init)
res_all_run_all <- res_all_run_all %>% filter(numIter>3)
gg <- ggplot(res_all_run_all,aes(x=numIter,y=loglik,colour = Init)) + geom_line(linewidth=1.3) + geom_point(size=1.5) 
gg<- gg + theme(axis.text=element_text(size=15),axis.title=element_text(size=15),legend.text = element_text(size=15))
gg  + theme(legend.title = element_text(size=15)) + 
  labs(y = "Log likelihood",
       x = "Iterations")

```






## 3. Problems of convergence 


### 3.a Simulation of data
In order to illustrate a bit more the behaviour of the EM algorithm, we consider the special case where the parameters $(p,\sigma^2_{1}, \sigma^2_2)$ are known  and fixed to their true value and we only want to estimate $(\mu_1,\mu_2)$. For simplicity, for this particular experiment, we simulated data $y$ with parameters $$p=0.36 \quad \sigma^2_2 = \sigma^2_1 = 9 \quad (\mu_1 , \mu_2) = (35,45).$$

We fix the variance to their true values and only estimate $\mu_1,\mu_2$. 

```{r  eval  = TRUE, echo = TRUE} 
theta_true = list(mu = c(35, 45), var = c(9,9), p = 0.36)
```


```{r  eval  = FALSE, echo = TRUE} 
Zsim = sample(c(1,2),length(y),prob=c(theta_true$p,1-theta_true$p),replace=TRUE)
ysim = theta_true$mu[Zsim] +sqrt(theta_true$var[Zsim])*rnorm(length(y))
```
```{r laod datasim,  eval  = TRUE, echo = FALSE}

load(file='datasim.Rdata')
```




### 3.b  Heatmap of the likelihood


When only estimating $\mu_1$ and $\mu_2$,  we can represent the 2 variables-function  $(\mu_1, \mu_2) \mapsto \log p_\theta(y)$ with a heat map as represented in the following figure. 


```{r heatmap, eval  = FALSE} 
N = 600
res_like = data.frame(mu1=double(),
                      mu2 = double(), 
                      ll = double())
grid_mu1 = seq(20,60,len=N)
grid_mu2 = seq(20,60,len=N)
grid_params <- as.data.frame(matrix(0,N*N,2))
names(grid_params) <- c('mu1', 'mu2') 
grid_params[,1]<-  rep(grid_mu1,N)
grid_params[,2] <- rep(grid_mu2,each=N)
res_loglik <- mclapply(1:N^2,function(i){
  params.i = theta_true
  params.i$mu <- c(grid_params[i,1],grid_params[i,2])
  logLikelihood(params.i,y=ysim)},mc.cores = 4)
res_like = grid_params
res_like[,3]  = unlist(res_loglik)
names(res_like)[3] <- 'loglik'
save(res_like, file = "loglik_heatmap.Rdata")
```

```{r load heatmap, eval  = TRUE, echo = FALSE} 
load("loglik_heatmap.Rdata")
```

Parameter maximizing the likelihood

```{r ll max, eval  = TRUE} 
grid_params <- res_like[,c(1,2)]
wmax = which.max(res_like[,3])
grid_params[wmax,]
```


### 3.b  EM starting from several points 


```{r EM start} 

theta0 = theta_true
params0 <- lapply(1:5,function(i){theta0})
params0[[1]]$mu = c(25,25)
params0[[2]]$mu = c(30,25)
params0[[3]]$mu = c(55,40)
params0[[4]]$mu = c(25,55)
params0[[5]]$mu = c(25,45)

res_all_run <-do.call(rbind,lapply(1:length(params0),function(run){
  res_EM_run  <- EM_2Mixture(params0[[run]],ysim,tol=10^{-5},estim_only_mu = TRUE)
  niter <- nrow(res_EM_run)
  res_EM_run <- cbind(res_EM_run,rep(run,niter))
  res_EM_run <- cbind(res_EM_run, 1:niter)
  names(res_EM_run)[7]='Init'
  names(res_EM_run)[8]='numIter'
  
  return(res_EM_run)}))
```


Final plot: 

```{r eval = TRUE, echo = FALSE} 


#------------ PLOT ------------------

final_points <- res_all_run %>%
  group_by(Init) %>%
  filter(numIter == max(numIter)) %>%
  filter(Init %in% c(1, 2, 4))

# Ajouter les log-vraisemblances finales au dataframe des points finaux
final_points <- final_points %>%
  mutate(loglik_final = format(round(loglik, 2), nsmall = 2)) # Formater les valeurs


res_all_run$Init <- as.factor(res_all_run$Init)
res_all_run$numIter <- as.factor(res_all_run$numIter)
true_mu <- as.data.frame(matrix(theta_true$mu,nrow=1))
names(true_mu)  =c('mu1','mu2')
gg <- ggplot(res_like,aes(x=mu1,y=mu2,z=loglik)) + geom_tile(aes(fill=loglik)) + scale_fill_viridis() + stat_contour(color="white", linewidth=0.25)
gg <- gg + geom_point(data=res_all_run, aes(x=mu1, y=mu2,z=NULL,colour=Init),size=3) + geom_line(data=res_all_run, aes(x=mu1, y=mu2,z=NULL,col=Init),linewidth=1.3) + scale_shape_manual(values=c(15, 20))
gg <- gg + geom_point(data=true_mu, aes(x=mu1, y=mu2,z=NULL),size=4,color='black') 
#gg <- gg+  geom_line(linewidth=1.3) + geom_point(size=1.5) 
gg<- gg + theme(axis.text=element_text(size=15),axis.title=element_text(size=15),legend.text = element_text(size=10))
gg <- gg  + theme(legend.title = element_text(size=15))+ guides(shape="none") + labs(x=expression(mu[1]),y=expression(mu[2]))

 
# Ajouter les annotations au graphique
gg <- gg + geom_text(data = final_points, 
            aes(x = mu1, y = mu2, label = loglik_final), 
            color = "black", size = 5, vjust = -1)
 



init_labels <-c(
  "1" = "Init 1 (25, 25)",
  "2" = "Init 2 (30, 25)",
  "3" = "Init 3 (55, 40)",
  "4" = "Init 4 (25, 55)",
  "5" = "Init 5 (25, 45)"
)

# Modifier la légende de Init en ajoutant les valeurs initiales
gg <- gg + 
  scale_colour_discrete(
    labels = init_labels, # Libellés personnalisés
    name = "Initialisation" # Titre de la légende
  )
gg
```



The true parameter $(\mu_1^{\star},\mu^{\star}_{2})$ is represented by a black point at coordinates $(35,45)$. It is visibly close to the global maximum of the likelihood.  In this figure we also observe a local maximum at coordinates about $(48,35)$ and a saddle point at around $(40,40)$. On the same plot, we plotted the trajectories of various EM algorihtm starting respectively at the values provided in the legend. The initial values $4$ and $5$  lead the EM to the global maximum ($-1197.48$). Starting from the initial points $2$ and $3$, the EM converges to a local maximum, while starting at the first initial value,  the algorithm reaches a saddle point.
 
 
 

## Conclusion

In practice the EM algorithm must be initialized on several points and/or well chosen points, trying to explore various regions of the parameter space.   


## References 