---
title: "Chapter 2.  Bohemia PalmerPinguins dataset"
author: "Sophie Donnet"
date: "2025-01-07"
bibliography: biblio.bib
output: 
  html_document:
   self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, message=FALSE, warning=FALSE}
library(ggplot2)
library(parallel)
library(viridis)
library(dplyr)
library(knitr)


require(abind)
require(mixtools)
require(parallel)


# Librairies --------------------------------------------------------------

library(tidyverse) # For data manipulation
library(GGally) # For ggpairs function
library(xtable) # To print teX tables

```

## 1.  The Meadow vegetation data 


We consider the data Data `data(ohrazeni)` from the R package `traitor`. This package is issued from the article "Community trait response to environment: disentangling species turnover vs intraspecific trait variability effects"  by Lepvs, de Bello, Vsmilauer and Dole (2011) @lepvs2011community. 



@lepvs2011community study the vegetation composition in meadows of Bohemia, Czech Republic. 
Four specific traits, namely the specific leaf area (SLA), the leaf dry matter content (LDMC), the reproductive plant height and the seed weight are measured over 58 species \footnotemark. 
 Our objective is the identify groups of species that share similar traits. An extract of the table corresponding to this dataset is provided here after. 
 
 
In this dataset, we have removed the non numerical variables and the outliers (huge seed weight). 
The resulting dataset is in the file `Bohemia_vegetation.csv.` 
 

```{r dataMeadow, message=FALSE, warning=FALSE}

library(readr)
donnees <- read.table("Bohemia_vegetation.csv",
                      header = TRUE, sep = ",") %>% 
  rename("Seed weight" = "Seed.weight")
view(donnees)
```


<img src="Meadow.jpeg" alt="Meadow in Bohemia" width="20%" />


 

We now plot de data. 





```{r data plot, message=FALSE, warning=FALSE}

# Descriptive figures -----------------------------------------------------
## Pair plot ---------------------------------------------------------------

pair_plot <- ggpairs(donnees %>% select_if(is.numeric), upper = list(continuous = wrap("points", 
                                                          size = 1/.pt)), lower = "blank") +
  theme_bw() +
  theme(text = element_text(size = 10),
        axis.text = element_blank(),
        axis.ticks = element_blank()) 
pair_plot
```          

## 2. EM algorithm for multivariate gaussian distributions 




\paragraph{Influence of the starting point}
First, we illustrate an important feature of the EM algorithm which is the influence of the starting parameter $\curpar[0]$. 
For $K = 3$, we chose randomly 200 different starting points and run the subsequent algorithms. 
The log-likelihood is monitored through the algorithm, and the algorithm stops when the increase in the log-likelihood becomes lower than $10^{-8}$.
Figure \ref{fig:gmm:log:likelihood} shows the differences in the final log-likelihood, therefore illustrating the numbers of local maxima. 
It is worth noting that this is not a problem \textit{per se}, as one can run (in parallel) the algorithm from multiple starting points, and choose the best based on the equation \eqref{eq:gmmLogLik}.
This however illustrates the difficulty to find a global maxima of the likelihood in complex settings.

\paragraph{Choosing the number of components.}
The previous procedure was performed for $K\in \lbrace 1,\dots, 6\rbrace$, and choosing the bet final point among the 200 trials\footnote{The case $K =1$ does not require any EM algorithm, has it boils down to the simple estimation of a mean a covariance matrix.}. 
For the 6 models, we compute the 3 model selection criterion discussed above, together with the negative log-likelihood and show the results on Figure \ref{fig:gmm:criterions}.
We can notice that the AIC finds a 4 components model to be the best, while the two other criteria lead to $K = 3$, which is kept as the final model in the following.

\paragraph{Clustering results.}
For $K = 3$, the best parameter (in the sense of Figure \ref{fig:gmm:log:likelihood}) is chosen.  
Estimator \eqref{eq:gmm:MAP:estimator} is then computed to cluster observations. 
The results are shown on Figure \ref{fig:gmm:cluster:results}.
The three clusters red green blue gather respectively 20, 7, and 31 observations.
The first one gathers observations having a low lead dry matter content while the second one gathers plants with high seed weight and high LDMC. 
It is worth noting that this cluster might contain an outlier (having a small SLA).
Gaussian mixture models are indeed popular models for anomaly detection (see \cite{chandola2009anomaly}, section 7).
The third cluster is a class having no clear specificity. 
Such cluster gathering together disparate observations often occur in clustering.


### 2.a Functions  


The useful functions are in the file `utils_gaussian_mixture_functions.R`. 
  
```{r source EM }
source('utils_gaussian_mixture_functions.R')
ys <- read.table("Bohemia_vegetation.csv",
                 header = TRUE, row.names = 1, sep = ",") %>%   as.matrix()
```




### 2.b Run the  EM  




We now run the EM for several numbers of clusters and several intialisation points. 



The code 
```{r starting points}
get_best_estimate <- function(ks, ys, n_trys){
  purrr::map(ks, function(k){
    if(k == 1){
      all_res_k <- get_EM_estimation_mixture(ys = ys, 
                                theta0 = list(pi = 1,
                                              mu = rep(0, ncol(ys)),
                                              Sigma = array(diag(1, ncol(ys)),
                                                            dim = c(ncol(ys), ncol(ys), 1))), 
                                n_iter_max = 2000)
      return(c(all_res_k, k = k))
    }
    else{
      all_res_k <- mclapply(1:n_trys, function(i){
        set.seed(i)
        initial_theta <- get_theta0(k = k, ys = ys)
        all_res <- try(get_EM_estimation_mixture(ys = ys, 
                                                 theta0 = initial_theta, 
                                                 n_iter_max = 2000))
        if(inherits(all_res, "try-error")){
          return(NULL)
        }
        else{
          return(c(all_res, seed = i))
        }
      }, mc.cores = detectCores() - 2)
      lls <- map_dbl(all_res_k, function(x){
        if(is.null(x)){
          return(-Inf)
        }
        else{
          return(x$final_criterions$log_lik)
        }})
      return(c(all_res_k[[which.max(lls)]], k = k))
    }
  })
}
```



```{r running estimation, eval = FALSE, echo =  TRUE}

gmm_est_multiple_ks <- get_best_estimate(1:6, ys, 100)
gmm_est_k3 <- mclapply(1:100, function(i){
  set.seed(i)
  initial_theta <- get_theta0(k = 3, ys = ys)
  all_res <- try(get_EM_estimation_mixture(ys = ys, 
                                           theta0 = initial_theta, 
                                           n_iter_max = 2000))
  if(inherits(all_res, "try-error")){
    return(NULL)
  }
  else{
    return(c(all_res, seed = i))
  }
}, mc.cores = detectCores() - 2)
gmm_est_best <- map_dbl(gmm_est_k3, function(x){
  if(is.null(x))
    return(-Inf)
  else
    return(x$final_criterions$log_lik)
}) %>% 
  which.max() %>% 
  gmm_est_k3[[.]]

save(list = ls(pattern = "gmm_est"),
     file = "gmm_est.RData")

```

```{r load estimation, eval = TRUE, echo =  FALSE }
load('gmm_est.RData')
```




## Conclusion

In practice the EM algorithm must be initialized on several points and/or well chosen points, trying to explore various regions of the parameter space.   


## References 