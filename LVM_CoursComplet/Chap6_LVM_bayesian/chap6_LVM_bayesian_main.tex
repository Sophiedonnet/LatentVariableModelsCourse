\documentclass[compress,10pt]{beamer}
% version imprimable pour assistance
%\documentclass[10pt, green, handout]{beamer}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} % le document est en français
\usepackage{rotating,amsmath}
\usepackage{graphicx,cancel}       % pour ins\'erer des figures
        % pour d\'efinir plus de couleurs
\usetheme{metropolis} 
\usepackage{xcolor,colortbl}
\usepackage{array}
\usepackage{mdframed}

\usepackage{lmodern}	



\definecolor{dgreen}{RGB}{235, 129, 27}
%\definecolor{lgreen}{RGB}{0,140,142}
%\definecolor{mygreen}{RGB}{20,176,61}

\definecolor{vert}{RGB}{147,196,125}
\definecolor{monorange}{RGB}{230,159,0}
\def \vert{\color{dgreen}}
\def \noir{\color{black}}
\def \rouge{\color{red}}


%\setbeamercolor{structure}{fg=INRA@dinst}

\setbeamertemplate{blocks}[rounded][shadow=true]
\setbeamercolor{block title}{use = structure , fg=dgreen}
%\setbeamercolor{normal text}{fg=black,bg=white}
%\setbeamercolor{alerted text}{fg=lgreen}
%\setbeamercolor{example text}{fg=lgreen}
%\setbeamercolor{structure}{fg=dgreen} %d'où ce bleu par défaut
%\setbeamercolor{background canvas}{parent=normal text}

\setbeamerfont{bibliography item}{size=\tiny}
\setbeamerfont{bibliography entry author}{size=\tiny}
\setbeamerfont{bibliography entry title}{size=\tiny}
\setbeamerfont{bibliography entry location}{size=\tiny}
\setbeamerfont{bibliography entry note}{size=\tiny}


\usetikzlibrary{calc,shapes,backgrounds,arrows,automata,shadows,positioning}
\usepackage{tikz}


%\addtobeamertemplate{navigation symbols}{}{%
%    \usebeamerfont{footline}%
%    \usebeamercolor[fg]{footline}%
%    \hspace{1em}%
%    \insertframenumber/\inserttotalframenumber
%}
%\pgfdeclareimage[height=\paperheight,width=\paperwidth]{intro}{plots/plante-insecte-ombre-COLLAGE.jpg}
%\setbeamertemplate{background canvas}{\pgfuseimage{intro}}

%\newmdenv[tikzsetting={draw=black, fill=white, fill opacity =0.7, line width= 4pt}, backgroundcolor=white, leftmargin=0, rightmargin=40,innertopmargin=4pt]{titlebox}


\setbeamertemplate{frametitlecontinuation}{\insertcontinuationcountroman}

%-------------------------------------------------------------------------------
% Quelques options pdf
%-------------------------------------------------------------------------------
\hypersetup{
pdfpagemode = FullScreen, % afficher le pdf en plein \'ecran
pdfauthor   = {},%
pdftitle    = {},%
pdfsubject  = {},%
pdfkeywords = {Science,Impact},%
pdfcreator  = {PDFLaTeX,emacs,AucTeX},%
pdfproducer = {INRA}%
}

\hypersetup{
    colorlinks=true,
    linkcolor=dgreen,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\newcommand\Wider[2][3em]{%
\makebox[\linewidth][c]{%
  \begin{minipage}{\dimexpr\textwidth+#1\relax}
  \raggedright#2
  \end{minipage}%
  }%
}

\AtBeginSection[]
{  \begin{frame}
  \frametitle{}
  \tableofcontents[currentsection, hideothersubsections]
  \end{frame} 
}
\AtBeginSubsection[]
{  \begin{frame}
  \frametitle{}
  \tableofcontents[currentsubsection, currentsection,hideothersubsections, subsectionstyle=show/shaded/hide]
  \end{frame} 
}
 
\newtheorem{proposition}{Proposition}
\newtheorem{algorithm}{Algorithm}
 
 
\graphicspath{{/home/sophie/Dropbox/WORK_DROPBOX/ENSEIGNEMENT/2024-Saclay-MathsSV/CoursVariablesLatentes/LVM_CoursComplet/Chap5_LVM_VAE/images}{/home/sophie/Dropbox/WORK_DROPBOX/ENSEIGNEMENT/2024-Saclay-MathsSV/CoursVariablesLatentes/LVM_CoursComplet/tools/logo/}}


    

  
\usepackage{subfig} 
%variables vectorielles
\usepackage{amsmath, setspace, amsfonts, amssymb, graphics,multirow,multicol}
\usepackage{interval}
%\graphicspath{{plotsChap3/}}


\title{Latent variable models in biology and ecology}%titre premiere page
\subtitle{\textbf{Chapter 6}: Bayesian inference for Latent variable models}
\author{Sophie  Donnet.  \includegraphics[scale=.1]{Logo-INRAE.jpg} }
%\author{Sophie  Donnet.  \includegraphics[scale=.1]{/home/sophie/Dropbox/WORK_DROPBOX/ENSEIGNEMENT/2022-Orsay-MathsSV/Slides_LVM_CoursComplet/tools/logo/Logo-INRAE.jpg} }



\date{ \textbf{Master 2 MathSV}. \today}

%\input{/home/sophie/Dropbox/WORK_DROPBOX/ENSEIGNEMENT/2022-Orsay-MathsSV/Slides_LVM_CoursComplet/tools/notations_maths.tex}
\input{/home/sophie/Dropbox/WORK_DROPBOX/ENSEIGNEMENT/2024-Saclay-MathsSV/CoursVariablesLatentes/LVM_CoursComplet/tools/notations_maths.tex}


%============================================
\begin{document}

%============================================
\begin{frame}
%============================================
\titlepage

\vspace{-3cm}
\begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}}c}
\includegraphics[scale=.2]{UPS.png}&
\includegraphics[scale=.08]{AgroParisTech.png}
%\includegraphics[scale=.2]{/home/sophie/Dropbox/WORK_DROPBOX/ENSEIGNEMENT/2022-Orsay-MathsSV/Slides_LVM_CoursComplet/tools/logo/UPS.png}&
%\includegraphics[scale=.08]{/home/sophie/Dropbox/WORK_DROPBOX/ENSEIGNEMENT/2022-Orsay-MathsSV/Slides_LVM_CoursComplet/tools/logo/AgroParisTech.png}

\end{tabular*}
\end{frame}




\input{basics.tex}

 

\input{MCMC.tex}

\input{variational.tex}

\input{importance_sampling.tex}


 %----------------------------------------------------------------------------- 
 
 
 
\section{Conclusion}


\begin{frame}{About latent variable models}
 
 \begin{itemize}
  \item Latent variables naturally arise is many models
  \item Require specific inference methods because 
  \begin{itemize}
  \item the likelihood is not explicit anymore (NLME) 
  \item the likelihood can not be computed in a reasonnable time (SBM) 
  \item we are interested in the posterior distribution of the latent variables $p(\bZ | \bY)$ (mixture models)
 \end{itemize}
\end{itemize}

\end{frame}

  %----------------------------------------------------------------------------- 
 
 \begin{frame}{About the Bayesian inference}
  
  \begin{itemize}
  \item MCMC are VERY flexible tools to infer latent variable models
  \item Universal package for ANY model
  \item \vert However \noir
  \begin{itemize}
  \item Reach their limit for models with large latent space. 
  \item For a complicated model the MCMC will  require tunnings to make it converge, SMC may be more efficient
  \end{itemize}
  
  \item People trying to propose universal tools for other methods to get the posterior distribution (\href{https://www.r-inla.org/}{\textcolor{dgreen}{INLA}}  for gaussian latent variable models for instance...) 
  \item New tools  gathering all the possibilities :  \href{https://mc-stan.org/}{Stan}, \href{https://cran.r-project.org/web/packages/LaplacesDemon/index.html}{LaplaceDemon}...
  \end{itemize}
  
  
 \end{frame}

 



\begin{frame}[allowframebreaks=0.90]{Références}
\bibliographystyle{apalike}
 \small{\bibliography{biblio2}}
  \end{frame}
  
  
  
\end{document}

  
  \section[]{Basics in probability}




\begin{frame}\label{basics probability}
\frametitle{Random variable with a finite support}
\begin{itemize}
\item  $Z$ taking possibly a finite number of values, say $\{1, \dots,K \}$. 
\item  Distribution described by  $$\left(P(Z=k)\right)_{k=1\dots K}$$ such that $\sum_{k=1}^K P(Z=k)=1$. \vert Probability mass function \noir 
\item Or equivalently, by $\left(P(Z\leq k)\right)_{k=1\dots K}$ \vert Cumulative distribution function \noir
\item \vert Expectation\noir:  $$E[Z] = \sum_{k=1}^K k (P(Z\leq k)$$
\item \vert Example\noir: Binomial distribution : number of successes in a flip coin game over $n$ trials
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Discrete random variable}
\begin{itemize}
\item  $Z$ taking possibly a infinite number of discrete values, say $\mathbb{N}$. 
\item Distribution described by $\left(P(Z=k)\right)_{k \in \mathbb{N}}$ such that $\sum_{k=1}^\infty P(Z=k)=1$. 
\item Or equivalently by $\left(P(Z\leq k)\right)_{k  \in \mathbb{N} }$
\item  \vert Expectation: \noir   $E[Z] = \sum_{k=1}^\infty k (P(Z\leq k)$
\item \vert Example\noir: Poisson distribution : $$\forall k \in \mathbb{N}, P(Z=k) = e^{-\lambda}\frac{\lambda^k}{k!}$$
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Continuous random variable}
\begin{itemize}
\item $Z$ taking possibly all the values of an interval (bounded or not)  say $\Omega$. 
\item $P(Z=x)=0$ $\forall x \in \Omega$
\item \vert Density probability function \noir $f : \R \mapsto \R^{+}$ such that 
\begin{eqnarray*}
f(x) &=& 0,\quad  \forall x  \notin \Omega\\
\int_{\Omega} f(x) dx&=&1\\
P(Z\leq x )  &=& \int_{-\infty}^x f(t) dt
\end{eqnarray*}

\item $F : \R \mapsto [0,1]$, $F(t) = \int_{-\infty}^t f(x) ds$ is the \vert cumulative distribution function. \noir
\item Its expectation is $E[Z] = \int t f(t) dt$
\item \vert Examples\noir: Gaussian, exponential,uniform,  Beta distribution
\end{itemize}




\hyperlink{Introduction}{\beamerreturnbutton{Back to presentation}}

\end{frame}


\subsubsection{Hypotesis testing}
\begin{frame}[allowframebreaks]\frametitle{Bayesian Hypotesis testing}


\begin{itemize}
\item Assume that
\begin{enumerate}
\item  we know that without treatment the hallucination rate is around 0.1. 
\item  we only observe individuals under treatment
\end{enumerate}
\item One interesting hypothesis testing would be
$$ (\mathcal{H}_0): \theta < 0.1 \quad \mbox{versus} \quad   (\mathcal{H}_1): \theta >= 0.1$$ 
\end{itemize}

\begin{block}{Bayes Factor}
The Bayes factor is equal to
\begin{eqnarray*}
 B_{10}^\pi &=&\left.\frac{\P(\theta \in \Theta_1 | \bY)}{\P(\theta \in \Theta_0 | \bY)}\middle/ \frac{\P(\theta \in \Theta_1)}{\P(\theta \in \Theta_0)}\right.\\
%&=& \left. \frac{\int_{\Theta_1} \pi(\theta | \bY)d\theta }{\int_{\Theta_0} \pi(\theta | \bY)d\theta}\middle/ \frac{\P(\theta \in \Theta_1)}{\P(\theta \in \Theta_0)}\right.
\end{eqnarray*}
where $\Theta_0 = [, 0.1]$ and $\Theta_1 = [0.1, 1]$.  
\end{block}


\end{frame}

%###########################################################
\begin{frame}[allowframebreaks]\frametitle{Remarks on the Bayes factor}

\begin{itemize}
\item The Bayes Factor is a likelihood ratio
\begin{eqnarray*}
 B_{10}^\pi &=&\left.\frac{\P(\theta \in \Theta_1 | \bY)}{\P(\theta \in \Theta_0 | \bY)}\middle/ \frac{\P(\theta \in \Theta_1)}{\P(\theta \in \Theta_0)}\right.\\
&=& \left. \frac{\int_{\Theta_1} \pi(\theta | \bY)d\theta }{\int_{\Theta_0} \pi(\theta | \bY)d\theta}\middle/ \frac{\P(\theta \in \Theta_1)}{\P(\theta \in \Theta_0)}\right.\\
&=& \left. \frac{\int_{\Theta_1} \frac{\ell(\bY | \theta)\pi(\theta)}{\cancel{p(\bY)}} d\theta }{\int_{\Theta_0}  \frac{\ell(\bY | \theta)\pi(\theta)}{\cancel{p(\bY)}}d\theta}\middle/ \frac{\P(\theta \in \Theta_1)}{\P(\theta \in \Theta_0)}\right.\\
&=& \left. \frac{\int_{\Theta_1} \ell(\bY | \theta)\pi(\theta)  d\theta }{\int_{\Theta_0} \ell(\bY | \theta)\pi(\theta)d\theta}\middle/ \frac{\P(\theta \in \Theta_1)}{\P(\theta \in \Theta_0)}\right.
\end{eqnarray*}
\item In classical statistics, the likelihood is maximized (likelihood ratio test), here it is integrated over the prior distribution. 
\end{itemize}

\end{frame}

%###########################################################
\begin{frame}[fragile]\frametitle{Bayes factor in our example :  Rcode}

\begin{verbatim}
 prior_theta_0 = pbeta(0.1,1,1)
 prior_theta_1 = 1- pbeta(0.1,1,1)
 post_theta_0 = pbeta(0.1,a_post,b_post) 
 post_theta_1 = 1-pbeta(0.1,a_post,b_post)
 B10 =  1/(prior_theta_1/prior_theta_0)*
 post_theta_1/post_theta_0
 [1] 1.527133
\end{verbatim}
\end{frame}




%###########################################################



\begin{frame}\frametitle{Interpretation of Bayes factors}
 
 Jeffreys' scale {\tiny [Jeffreys (1961)]  } {\tiny [Kass and Raftery (1995)] }
 

 $$
\begin{array}{|l|c|}
   \hline
\log_{10}(B_{10}) & \mbox{Degree  of evidence in favor of Hypothesis }\\
\hline 
  \mbox{from } 0 \mbox{ to } 0.5 & \mbox{weak}     \\
 \mbox{from }  0.5 \mbox{  to }  1 &  \mbox{substantial}  \\
 \mbox{from }  1 \mbox{  to } 2   & \mbox{strong	} \\
  >2 & \mbox{Decisive}\\
   \hline
\end{array}$$


\textbf{Remarks}: 
\begin{itemize}
\item Far from being  justified on strict principles
\item $B_{10}=1/ B_{01}$: $\mathcal{H}_0$ and $\mathcal{H}_1$ are symetric.
\end{itemize}
\end{frame}


\subsubsection{Model Selection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]\frametitle{Model Selection}


\begin{itemize}
\item Assume that the observations come from two populations : one taking medecine and the other taking placebo  
\item We introduce the following notations: $\forall i=1,\dots n$: 
$$Z_i =\left\{ \begin{array}{cc} 
1 & \mbox{if  individual $i$ takes the medecine}\\
0 &  \mbox{if  individual $i$ takes the placebo}
\end{array}
\right.
$$
\item Two models in competition:  

$$(\mathcal{M}_1) \; 
Y_i \sim \mathcal{B}ern(\theta), \quad \quad (\mathcal{M}_2)\left\{ \begin{array}{cc} 
 Y_i|Z_i=0 &\sim \mathcal{B}ern(\theta_0)\\
 Y_i|Z_i=1 &\sim \mathcal{B}ern(\theta_1)
 \end{array}\right.
$$
with $\theta_0 \neq \theta_1$
\end{itemize}


\begin{block}{Question}
Which model is the most probable, given the data we observe?
\end{block}

\end{frame}

%###########################################################
\begin{frame}\frametitle{Model Selection}

\begin{itemize}
\item Model $\mathcal{M}_2$ has already been studied
\item In model $\mathcal{M}_2$ we have to unknown parameters  $(\theta_0,\theta_1)$
\item Prior on $(\theta_0,\theta_1)$ : $\pi(\theta_0,\theta_1) = \ind_{[0,1]}(\theta_0) \ind_{[0,1]}(\theta_1)$
\item Likelihood function
\begin{eqnarray*}
\ell(\bY; \theta_0, \theta_1) &=& \prod_{i=1, Z_i=0}^{n_0} \theta_0^{Y_i}(1-\theta_0)^{1-Y_i} \prod_{i=1, Z_i=1}^{n_1} \theta_1^{Y_i}(1-\theta_1)^{1-Y_i}\\
&=& \theta_0^{n_{10}}(1-\theta_0)^{n_0-n_{10}}   \theta_1^{n_{11}}(1-\theta_1)^{n_1-n_{11}}
\end{eqnarray*}
where 
\begin{itemize}
\item $n_{11}$ is the number of individuals with treatment and with hallucinations,
\item  $n_{10}$ is the number of individuals without treatment and with hallucinations,
\item  $n_1$ is the number of individuals with treatment, 
\item $n_0$ is the number of individuals without treatment. 
\end{itemize}
\item Posterior distribution : we recognize $2$ independent beta distributions
$$ \theta_1 | \bY \sim \mathcal{B}eta(n_{11}, n_1-n_{11}) \quad   \theta_0 | \bY \sim \mathcal{B}eta(n_{10}, n_0-n_{10}) $$

\end{itemize}
\end{frame}
%###########################################################

\begin{frame}\frametitle{Bayes Factor for model selection}

\begin{itemize}
\item Put a prior probability on each model $P(\mathcal M _1)$  and $P(\mathcal M _2)$ such that $P(\mathcal M _1)+ P(\mathcal M _2)=1$
\item We define Bayes factor as
$$ B_{21} = \left.\frac{\P(\mathcal{M}_2| \bY)}{\P(\mathcal{M}_1 | \bY)}\middle/ \frac{\P(\mathcal{M}_1)}{\P(\mathcal{M}_2)}\right.
$$

where
\begin{eqnarray*}
\P(\mathcal{M}_2| \bY)  =  \frac{P(\bY | \mathcal{M}_2)P(\mathcal{M}_2)}{P(\bY | \mathcal{M}_2)P(\mathcal{M}_2) + P(\bY | \mathcal{M}_1)P(\mathcal{M}_1)}
\end{eqnarray*}
\item And so : 
$$ B_{21} =  \frac{\P(\bY|\mathcal{M}_2)}{\P( \bY | \mathcal{M}_1)}  $$ 
with, in our case : 
$$ \P( \bY | \mathcal{M}_1) = \int \ell_1(\bY| \theta) \pi_1(\theta) d\theta$$ 
$$  \P( \bY | \mathcal{M}_2) = \int \ell_2(\bY| \theta_0, \theta_1) \pi_2(\theta_0,\theta_1)  d\theta_0 d\theta_1$$ 
\end{itemize}
\end{frame}
%###########################################################

\begin{frame}\frametitle{Remarks on Bayes Factor for model selection}

\begin{itemize}
\item Finally I need to be able to compute the normalizing constant (marginal likelihood) for model selection! 
\item ``Naturally'', Bayes factor will encourage the simplest models {\tiny \vert [Berger and Jefferys (1991)] \noir}
\item Automatic penalization due to the integration over the parameters 
\item Asymptotic approximation : when $n \rightarrow \infty$
Laplace development : \vert   $\Rightarrow$ \noir BIC criteria
 
 $$\log (B_{21})  \approx  \log \frac{\ell_2(\bY|\hat{\theta_2})} {\ell_1(\bY|\hat{\theta_1})} - \frac{p_2-p_1}{2}\log(n) $$ %+ K(\hat{\theta_1},\hat{\theta_2})$$

\small where $p_k$ is the number of parameters of model $\mathcal{M}_k$ and $\hat{\theta_k}$ is the maximum likelihood estimator of model $\mathcal{M}_k$. \normalsize
\item \vert But \noir  :  requires the MLE, does not take into account the prior, only in regular models, definitions of $n$ and $p$ can be difficult  when the data are not i.i.d.  \normalsize
\end{itemize}
\end{frame}


 \hyperlink{TakeHOME}{\beamergotobutton{B ack to presentation}}

  
\end{document}
% 
% 
% \section{Modèle de  mélange}
% 
% %----------------------------------------------------------------------------- 
%    \begin{frame}\frametitle{Modèle de mélange}
%    \begin{itemize}
%   \item \vert Hypothèses  \noir 
% \begin{enumerate}
% \item Les individus sont indépendants
% \item Les symptômes sont indépendants
% \item Les individus appartiennent à des groupes $g=1\dots G$ qui conditionnent leurs distributions
% \end{enumerate}
% \item On introduit 
% %$$Z_{ig} = \left\{\begin{array}{cl} 1 & \mbox{si le patient $i$ appartient au groupe  $g$} \\  0 & \mbox{sinon.} \end{array}\right.$$    
% %ou bien
%  \begin{itemize}
%   \item $$Z_{i} = \left.\begin{array}{cl} g & \mbox{si le patient $i$ appartient au groupe  $g$}  \end{array}\right.$$    
% \end{itemize}
% \item Alors \vert (modèle de mélange) \noir $\forall i=1\dots n, \forall k=1\dots K$ 
% $$\left\{ \begin{array}{cl}
% Y_{ik} | Z_i=g &\sim \mathcal{B}er(p_{kg})\\
% Z_i &\sim _{i.i.d}  \mathcal{M}_G(1; \pi_1,\dots,\pi_G)
% \end{array}
% \right.
% $$
% où $\mathcal{M}_G(1; \pi_1,\dots,\pi_G)$ est la distribution multinomiale à $G$ modalités et $1$ observation: $$\P(Z_i=g)=\pi_g, \quad \mbox{et} \quad \sum_{g=1}^G \pi_g = 1$$
%  \end{itemize}  
% \end{frame}
% %----------------------------------------------------------------------------- 
%       
% \begin{frame}\frametitle{Vraisemblance} 
% 
% \begin{itemize}
% \item \vert Paramètres: \noir$\theta = (p_{kg},\pi_g)_{k=1\dots K, g=1\dots G}$ 
% \item \vert Vraisemblance: \noir 
% \begin{itemize}
% \item $[Y_{ik}| \theta] = \sum_{g=1}^G \pi_g p_{kg}^{Y_{ik}}(1- p_{kg})^{1-Y_{ik}} $
% \item \begin{eqnarray*}
% [\bY | \theta] = \prod_{i=1}^n \prod_{k=1}^K \sum_{g=1}^G \pi_g p_{kg}^{Y_{ik}}(1- p_{kg})^{1-Y_{ik}} 
% \end{eqnarray*}
%  \end{itemize}
%  \end{itemize}
%  \end{frame}
%  %----------------------------------------------------------------------------- 
%  
% \begin{frame}{Loi  a priori}
% 
%  \begin{eqnarray*}
%  p_{kg} & \sim & \mathcal{B}eta(a_{kg},b_{kg}) \\
%  (\pi_1,\dots,\pi_G)&\sim & \mathcal{D}ir(\alpha_1,\dots,\alpha_G) \\
%  \end{eqnarray*}
%  
%  \begin{block}{où $\mathcal{D}ir$ est la loi de Dirichlet }
% \begin{itemize}
% \item Support : simplexe $\mathcal{S}_G = \left\{(\pi_1,\dots,\pi_{G-1},\pi_G)  \in [0,1] ^{G} | \sum_{g=1}^{G-1} \pi_g < 1, \pi_G = 1- \sum_{g=1}^{G-1} \pi_g\right\}$ 
% \item $[\pi_1,\dots,\pi_G] \propto \prod_{g=1}^G \pi_g^{\alpha_g-1}$ avec $\pi_G = 1 - \sum_{g=1}^{G-1} \pi_g$
% \item $E[\pi_g] = \frac{\alpha_g}{\alpha_0}$ où $\alpha_0 = \sum_{g=1}^G \alpha_g$  
% \item $Var[\pi_g] = \frac{\alpha_g(\alpha_0-\alpha_g)}{\alpha_0^2 (\alpha_0+1)}$   
% \item Uniforme: $\alpha_1= \dots= \alpha_G=1$
% \item Non informative au sens de Jeffreys: $\alpha_1= \dots= \alpha_G=1/2$
% \end{itemize}
% \end{block}
% 
% \end{frame}
% 
% %----------------------------------------------------------------------------- 
% \begin{frame}{Loi  a posteriori}
% 
% \begin{itemize}
% \item 
% 
% \begin{eqnarray*}
% [\theta | \bY] &\propto&   [\bY | \theta] [\theta]\\
% &\propto & \left[ \prod_{i=1}^n \prod_{k=1}^K \sum_{g=1}^G \pi_g p_{kg}^{Y_{ik}}(1- p_{kg})^{1-Y_{ik}}\right]  \prod_{g,k=1}^{G,K} \ind_{[0,1]}(p_{kg})  \\
% && \prod_{g=1}^G \pi_g^{\alpha_g-1}\ind_{\mathcal{S}_G}(\pi_1,\dots,\pi_G)  
% \end{eqnarray*}
% \item  Modèle non conjugué, loi a posteriori non explicite dans ce cas
% \item  \vert Moyenne a posteriori: \noir approximation  numérique de $\int \theta [\theta | \bY] d\theta$?  
% \item Approche \vert Monte Carlo \noir 
% \begin{itemize}
% \item Soit $\theta^{(1)},\dots, \theta^{(M)}$ $M$-échantillon de $[\theta | \bY]$  alors $(\theta^{(1)}\dots,\theta^{(M)})$ fournit une bonne approximation de $[\theta | \bY]$ (théorème de Glivenko-Cantelli dans $\R$)
% \item \emph{Loi des grands nombres} : $\frac{1}{M} \sum_{m=1}^M  \theta^{(m)}$ converge$^*$ vers $E[\theta | \bY]$
% \end{itemize}
% \end{itemize}
% \end{frame}
% 
% 
% %----------------------------------------------------------------------------- 
% \begin{frame}{Génération de $[\theta | \bY]$}
% 
% \begin{itemize}
% \item Génération directe impossible : on n'a pas reconnu de loi
% 
% \item On n'a pas exploité l'existence des données latentes (idée EM)
% $$\bZ = (Z_1,\dots ,Z_n) \in \{1,\dots G\}^n$$ 
% \end{itemize}
% \begin{block}{Remarque}Si on obtient un échantillon de $$(\theta^{(1)},\bZ^{(1)}), \dots, (\theta^{(M)},\bZ^{(M)}) \sim [\theta, \bZ | \bY]$$  alors \vert marginalement \noir 
% $$\theta^{(1)},\dots \theta^{(M)} \sim [\theta | \bY]$$
% \end{block}
% 
% \end{frame}
% 
% 
% 
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Génération de $[\theta ,\bZ| \bY]$}
% \begin{itemize}
% \item \begin{eqnarray*}
%  [\theta, \bZ | \bY] &\propto& [\bY | \bZ, \theta] [\bZ, \theta] = [\bY | \bZ, \theta] [\bZ |  \theta] [\theta]\\
%  &\propto& \left[\prod_{i=1}^n \prod_{k=1}^K p_{k Z_i}^{Y_{ik}}(1- p_{kZ_i})^{1-Y_{ik}}\right] \left[\prod_{i=1}^n \pi_{Z_i}\right]  
%   \prod_{k=1}^K  \prod_{g=1}^G   \ind_{[0,1]}(p_{kg})  \\&& \prod_{g=1}^G \pi_g^{\alpha_g-1}\ind_{\mathcal{S}_G}(\pi_1,\dots,\pi_G)  
%  \end{eqnarray*}
% \item \vert Pas de loi connue pour $(\theta,\bZ)$
% \end{itemize}
% 
% \end{frame}
% 
% 
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Lois conditionnelles  $[\theta  | \bZ, \bY]$ et $[\bZ  | \btheta, \bY]$  }
% 
%  {\scriptsize \begin{eqnarray*}
%  [\theta | \bZ,  \bY] &\propto& [\bY | \bZ, \theta] [\bZ, \theta] = [\bY | \bZ, \theta] [\bZ |  \theta] [\theta]\\
%  &\propto& \left[\prod_{i=1}^n \prod_{k=1}^K p_{k Z_i}^{Y_{ik}}(1- p_{kZ_i})^{1-Y_{ik}}\right] \left[\prod_{i=1}^n \pi_{Z_i}\right]  
%   \prod_{k=1}^K  \prod_{g=1}^G   \ind_{[0,1]}(p_{kg})  \\&& \prod_{g=1}^G \pi_g^{\alpha_g-1}\ind_{\mathcal{S}_G}(\pi_1,\dots,\pi_G)   \\
%   &\propto&   \left[ \prod_{k=1}^K\prod_{g=1}^G \prod_{i=1, Z_i=g}^n p_{kg}^{Y_{ik}}(1- p_{kg})^{1-Y_{ik}}\right] \left[\prod_{g=1}^n \prod_{i=1, Z_i=g}^n \pi_{g}\right] \\ 
% &&  \prod_{k=1}^K  \prod_{g=1}^G   \ind_{[0,1]}(p_{kg})   \prod_{g=1}^G \pi_g^{\alpha_g-1}\ind_{\mathcal{S}_G}(\pi_1,\dots,\pi_G)  \\
% &\propto&  \vert \prod_{k=1}^K \prod_{g=1}^G \underbrace{p_{kg}^{n_{kg}}(1- p_{kg})^{n_g-n_{kg}}  \ind_{[0,1]}(p_{kg})}_{\mathcal{B}(n_{kg}+1, n_g-n_{kg}+1)} \rouge  \underbrace{\prod_{g=1}^n   \pi_{g}^{n_g} \pi_g^{\alpha_g-1}\ind_{\mathcal{S}_G}(\pi_1,\dots,\pi_G)}_{\mathcal{D}(n_{1}+\alpha_1, \dots, n_G+\alpha_G)} 
%  \end{eqnarray*}
%  avec $n_g$ le nombre d'individus du groupe $g$ et $n_{kg}$ le nombre d'individus du groupe $g$ présentant le symptôme $k$. 
% }
% \end{frame}
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Loi  conditionnelle   $[\theta  | \bZ, \bY]$  }
% \begin{block}{$[\theta  | \bZ, \bY]$ }
% \begin{itemize}
% \item  $$ p_{kg} | \bZ,\bY  \sim \mathcal{B}eta(n_{kg}+1, n_g-n_{kg}+1), \forall (k,g)$$
% avec $n_g = \sum_{i=1}^n \ind_{Z_i=g}$ et  $n_{kg} = \sum_{i=1}^n \ind_{Z_i=g}Y_{ik}$
% \item $$ (\pi_1,\dots, \pi_{G})| \bZ,\bY  \sim \mathcal{D}ir(n_{1}+\alpha_1, \dots, n_G+\alpha_G) $$
% \end{itemize}
% \end{block}
% \end{frame}
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Loi  conditionnelle  $[\bZ | \theta, \bY]$  }
% 
% On cherche $\forall i=1,\dots, n, \forall g=1\dots G$,
% $$ \P(Z_i=g | \bY, \theta)$$ 
% 
% $\forall g=1\dots G$, 
% \begin{eqnarray*}
% \P(Z_i=g |\bY,\theta)  &=&  \P(Z_i=g |Y_i,\theta) \propto   \P(Y_i | Z_i=g, \theta) \P(Z_i=g |\theta)\\
% & \rouge \propto \noir &\left[ \prod_{k=1}^K p_{kg}^{Y_{ik}} (1-p_{kg})^{1-Y_{ik}}\right] \pi_g  := w_{ig}\\
% \P(Z_i=g |\bY,\theta)   &=& \frac{w_{ig}}{\sum_{g=1}^G w_{ig}}
% \end{eqnarray*}
% \end{frame}
% 
% 
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Lois conditionnelles  $[\theta  | \bZ, \bY]$  et $[\bZ | \theta, \bY]$ }
% \begin{block}{$[\theta  | \bZ, \bY]$ }
% \begin{itemize}
% \item  $$ p_{kg} | \bZ,\bY  \sim \mathcal{B}eta(n_{kg}+1, n_g-n_{kg}+1), \forall (k,g)$$
% avec $n_g = \sum_{i=1}^n \ind_{Z_i=g}$ et  $n_{kg} = \sum_{i=1}^n \ind_{Z_i=g}Y_{ik}$
% \item $$ (\pi_1,\dots, \pi_{G})| \bZ,\bY  \sim \mathcal{D}ir(n_{1}+\alpha_1, \dots, n_G+\alpha_G) $$
% \end{itemize}
% \end{block}
% 
% 
% \begin{block}{$[\bZ | \theta, \bY]$ }
% \begin{eqnarray*}
% \P(Z_i=g |\bY,\theta)&  \propto  &\left[ \prod_{k=1}^K p_{kg}^{Y_{ik}} (1-p_{kg})^{1-Y_{ik}}\right] \pi_g  := w_{ig},  \quad   g=1,\dots ,G\\
% \P(Z_i=g |\bY,\theta)   &=& \frac{w_{ig}}{\sum_{g=1}^G w_{ig}}
% \end{eqnarray*}
% \end{block}
% 
% 
% \end{frame}
% 
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Algorithme de Gibbs}
% 
% \begin{block}{}
% \begin{enumerate}
% \item[] \vert Itération 0:  \noir Initialiser $\theta^{(0)}$  et $\bZ^{(0)}$ (par un EM par exemple) 
% \item[] \vert Itération $m$ $(m=1\dots M)$: \noir  Etant données des valeurs courantes de $\bZ^{(m-1)}$, $\theta^{(m-1)}$
% \begin{itemize}
% \item Simuler $\bZ^{(m)} \sim [\bZ| \theta^{(m-1)}, \bY]$
% $\forall i=1,\dots,n$, $\forall g=1,\dots, G$
% {\scriptsize 
% \begin{eqnarray*}
% \P(Z^{(m)}_i=g |\bY,\theta^{(m-1)})&  \propto  &\left[ \prod_{k=1}^K (p^{(m-1)}_{kg})^{Y_{ik}} (1-p^{(m-1)}_{kg})^{1-Y_{ik}}\right] \pi^{(m-1)}_g \\
% && := w^{(m-1)}_{ig},\\ %\quad   g=1,\dots ,G\\
% \P(Z^{(m)}_i=g |\bY,\theta^{(m-1)})   &=& \frac{w^{(m-1)}_{ig}}{\sum_{g=1}^G w^{(m-1)}_{ig}}
% \end{eqnarray*}
% }
% \item Simuler  $\theta^{(m)} \sim [\theta | \bZ^{(m)}, \bY]$
% {\scriptsize 
% \begin{itemize}
% \item  $n^{(m)}_g = \sum_{i=1}^n \ind_{Z^{(m)}_i=g}$ et  $n^{(m)}_{kg} = \sum_{i=1}^n \ind_{Z^{(m)}_i=g}Y_{ik}$
% \item  $ p^{(m)}_{kg} | \bZ^{(m)},\bY  \sim \mathcal{B}eta(n^{(m)}_{kg}+1, n^{(m)}_g-n^{(m)}_{kg}+1), \forall (k,g)$
% \item $ (\pi^{(m)}_1,\dots, \pi^{(m)}_{G})| \bZ,\bY  \sim \mathcal{D}ir(n^{(m)}_{1}+\alpha_1, \dots, n^{(m)}_G+\alpha_G) $
% \end{itemize}}
% \end{itemize}
% \end{enumerate}
% \end{block}
% \end{frame}
% 
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Propriétés de la chaîne $(\bZ^{(m)}$, $\theta^{(m)})_{m\geq 0}$}
% \begin{block}{}
%  $(\bZ^{(m)}$, $\theta^{(m)})_{m\geq 0}$ est un processus de Markov à temps discrets $(m)$, à espace d'états continu
%  \begin{itemize}
% \item $(\bZ^{(m)}$, $\theta^{(m)})$ ne dépend que de l'itération précédente (et des données $\bY$)
% \end{itemize}
% \end{block}
% 
% \begin{block}{}
% Elle admet comme loi stationnaire $[\bZ,\theta | \bY]$ 
% \begin{itemize}
% \item i.e. si $(\bZ^{(m)}$, $\theta^{(m)}) \sim [\bZ,\theta | \bY]$ $(\bZ^{(m+1)}$, alors  $\theta^{(m+1)}) \sim [\bZ,\theta | \bY]$
% \end{itemize}
% \end{block}
% 
% \begin{block}{Convergence}
% \emph{Rappel sur les chaînes de Markov}: Si la chaîne de Markov est irréductible (tous les états communiquent entre eux), récurrente (proba de retour >0), positive (existence d'une loi stationnaire)  et apériodique alors elle converge vers son unique loi stationnaire. 
% \begin{itemize}
% \item C'est le cas ici (en général vrai, sauf cas très particulier) 
% \item Pour $m$ très grand, $(\bZ^{(m)}$, $\theta^{(m)}) \sim [\bZ,\theta | \bY]$
% \end{itemize}
% \end{block}
% \end{frame}
% 
% %----------------------------------------------------------------------------- 
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Algorithme de Gibbs en général}
% 
% Si on cherche à échantillonner une loi du type $g(x_1,\dots,x_p)$  telle que toutes les lois conditionnelles $g_j(x_j | x_{\{-j\}})$  sont explicites alors l'algorithme de Gibbs s'écrit : 
% \begin{block}{}
% \begin{enumerate}
% \item[] \vert Itération 0:  \noir Initialiser $x_1^{(0)} \dots, x_p^{(0)} $ 
% \item[] \vert Itération $m$ $(m=1\dots M)$: \noir  Etant données des valeurs courantes de  $x_1^{(m-1)}, \dots, x_p^{(m-1)}$, 
% \begin{itemize}
% \item Simuler $x^{(m)}_1 \sim g_1(x_1 | x_2^{(m-1)}, \dots,x_p^{(m-1)} )$
% \item Simuler $x^{(m)}_2 \sim g_2(x_2 | x_1^{(m)}, x_3^{(m-1)}\dots,x_p^{(m-1)} )$
% \item Simuler $x^{(m)}_3 \sim g_3(x_3 | x_1^{(m)},  x_2^{(m)}, x_4^{(m-1)}\dots,x_p^{(m-1)} )$
% \item $\cdots$
% \item  Simuler $x^{(m)}_p \sim g_p(x_p | x_1^{(m)}, \dots,x_{p-1}^{(m )} )$
% \end{itemize}
% \end{enumerate}
% \end{block}
% La loi stationnaire est bien la loi jointe $g(x_1,\dots,x_p)$
% \end{frame}
% %----------------------------------------------------------------------------- 
% \begin{frame}\frametitle{Remarques sur l'échantillonnage de Gibbs }
% \begin{itemize}
% \item Nécessairement multidimensionnel (quitte à introduire des composantes du vecteur simulé artificiellement, $\bZ$ ici)
% \item Ne fonctionne pas si le nombre de paramètres est variable (ici, nombre de groupe $G$ fixé)
% \item Contraignant sur les modèles (lois conditionnelles explicites)
% \end{itemize}
% \end{frame}
% %----------------------------------------------------------------------------- 
% 
% %----------------------------------------------------------------------------- 
% \begin{frame}[fragile]\frametitle{En pratique : Code R}
% \begin{block}{}
% \begin{verbatim} 
% n = length(Znum); J = ncol(p); K = length(pi)
% Z = vapply(1:K, function(k){as.numeric(Znum==k)}, rep(1,n))
% for (b in 1:B){
%      N = colSums(Z)
%      pi = rdirichlet(1, N + d.prior)
%      p = matrix(rbeta(K*J, t(Z)%*%Y + a.prior,  
%        t(Z)*(Y==0) + b.prior), K, J)    
%       tau = exp((Y%*%t(log(p)) + 
%        (1-Y)%*%t(log(1-p)) + rep(1, n)%*%log(pi))
%       tau = tau/rowSums(tau)
%       Z = t(sapply(1:n,
%        function(i){rmultinom(1, 1, tau[i, ])}))
%    }
%    \end{verbatim}
% 
% \end{block}
% \end{frame}
% 
% 
% 
% %%%Local Variables:
% %%% mode: latex
% %%% eval: (TeX-PDF-mode 1)
% %%% ispell-local-dictionary: "francais"
% %%% eval: (flyspell-mode 1)
% %%%End:
% 

